{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assets/stopwords.pkl','rb') as f:\n",
    "    stopwords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_stop = ['shooting', 'guns', 'gun', 'shoot', 'bullets', 'bullet', 'shootings', \n",
    "            'clip', 'bar', 'college','student','students', 'magazine', 'ammo', 'ammunition','shootings',\n",
    "'rampage', \n",
    "'gunman', \n",
    "'stabbing', \n",
    "'slayings', \n",
    "'shooter', \n",
    "'fatally', \n",
    "'killings', \n",
    "'killing', \n",
    "'shot', \n",
    "'unarmed', \n",
    "'massacre', \n",
    "'slaying', \n",
    "'incident', \n",
    "'stabbings',\n",
    "'altercation',\n",
    "'shooters', \n",
    "'murder', \n",
    "'shoot', \n",
    "'carjacking'\n",
    "'gunned', \n",
    "'murders', \n",
    "'firing', \n",
    "'newtown',\n",
    "'scuffle', 'gunmen', 'florida', 'keys', 'key', 'school', 'ian', 'david','texas', 'miami', 'california', 'harvey', 'thousandoaks',\n",
    "           'assault','houston', 'ventura', 'san','cudjoe', 'oaks', 'en', 'de', 'la', \n",
    "            'el', 'tiroteo', 'th','massshooting','californiashooting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(gun_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('../project_4/assets/combined_edit_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98450, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_df['disaster']\n",
    "\n",
    "# Set X as text column.\n",
    "X = combined_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y, \n",
    "                                                    test_size=.30,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stopwords, \n",
    "                        max_df=0.95, \n",
    "                        min_df=5, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslee/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['beaufort', 'bermuda', 'dust', 'high', 'low', 'pressure', 'richter', 'scale', 'triangle', 'vane', 'violent'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslee/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "model = lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Training score: 0.8550097946745991\n",
      "LogReg Testing score: 0.8557982055188759\n"
     ]
    }
   ],
   "source": [
    "print(f'LogReg Training score: {model.score(X_train_tfidf, y_train)}')\n",
    "print(f'LogReg Testing score: {model.score(X_test_tfidf, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_tfidf = pd.SparseDataFrame(X_train_tfidf,\n",
    "                             columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68915, 1000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>active</th>\n",
       "      <th>actually</th>\n",
       "      <th>added</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.303191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abc  able  absolutely  access  accident  across  act  active  actually  \\\n",
       "0  NaN   NaN         NaN     NaN       NaN     NaN  NaN     NaN       NaN   \n",
       "1  NaN   NaN         NaN     NaN       NaN     NaN  NaN     NaN       NaN   \n",
       "2  NaN   NaN         NaN     NaN       NaN     NaN  NaN     NaN       NaN   \n",
       "3  NaN   NaN         NaN     NaN       NaN     NaN  NaN     NaN       NaN   \n",
       "4  NaN   NaN         NaN     NaN       NaN     NaN  NaN     NaN       NaN   \n",
       "\n",
       "   added  ...  year  years  yes  yesterday  yet        yo  york  young  \\\n",
       "0    NaN  ...   NaN    NaN  NaN        NaN  NaN       NaN   NaN    NaN   \n",
       "1    NaN  ...   NaN    NaN  NaN        NaN  NaN       NaN   NaN    NaN   \n",
       "2    NaN  ...   NaN    NaN  NaN        NaN  NaN       NaN   NaN    NaN   \n",
       "3    NaN  ...   NaN    NaN  NaN        NaN  NaN  0.303191   NaN    NaN   \n",
       "4    NaN  ...   NaN    NaN  NaN        NaN  NaN       NaN   NaN    NaN   \n",
       "\n",
       "   youtube  zone  \n",
       "0      NaN   NaN  \n",
       "1      NaN   NaN  \n",
       "2      NaN   NaN  \n",
       "3      NaN   NaN  \n",
       "4      NaN   NaN  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_tfidf.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df_tfidf = pd.SparseDataFrame(X_test_tfidf,\n",
    "                                    columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29535, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_test_df_tfidf.fillna(0, inplace=True)\n",
    "print(X_test_df_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslee/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['beaufort', 'bermuda', 'dust', 'high', 'low', 'pressure', 'richter', 'scale', 'triangle', 'vane', 'violent'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: thousand mass people killed dead pic victims least borderline go power un grill traffic many stop take long another southern\n",
      "Topic #1: tx know get ever let st show park photo birthday posted drinking bay fun closed downtown party mom watching care\n",
      "Topic #2: landfall love category makes mph pic made winds lower update fema percent gone begins well says making weekend surge state\n",
      "Topic #3: pic eye prayers south best eyewall hits everyone evacuation real next mandatory may west reaches thoughts better latest lower evacuations\n",
      "Topic #4: today tonight stay city largo open way heart world year time much still pic feel multiple safe goes shows high\n",
      "Topic #5: new live good really hit one watch center updates could hard work pic near post make west bad area times\n",
      "Topic #6: day like great got look happy morning pic getting think sunday ready game always even lol thanks around beautiful guys\n",
      "Topic #7: west man would everything someone us ahead pic looking hate abc toll lot fox face reports nothing westside read beach\n",
      "Topic #8: first want repost right back night please come thank friends last tomorrow going family cat pray week get coming music\n",
      "Topic #9: residents via youtube homes destroyed need video home house return pic cnn see evacuate west wait little big never thing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords)\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf = tf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=42)\n",
    "\n",
    "lda_train = lda.fit_transform(X_train_tf)\n",
    "lda_test = lda.transform(X_test_tf)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_loadings = pd.DataFrame(lda.components_, \n",
    "                                columns = tf_vectorizer.get_feature_names(),\n",
    "                                index = [f'topic_{x}' for x in range(lda.components_.shape[0])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_loadings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>west</th>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100044</td>\n",
       "      <td>0.100040</td>\n",
       "      <td>213.052667</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>152.841279</td>\n",
       "      <td>101.547642</td>\n",
       "      <td>340.233053</td>\n",
       "      <td>37.983505</td>\n",
       "      <td>205.201675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>278.123742</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100014</td>\n",
       "      <td>240.635450</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everything</th>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>232.059128</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someone</th>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>229.937012</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.100010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>80.650146</td>\n",
       "      <td>0.100023</td>\n",
       "      <td>13.109711</td>\n",
       "      <td>52.283461</td>\n",
       "      <td>40.281836</td>\n",
       "      <td>30.984174</td>\n",
       "      <td>0.100026</td>\n",
       "      <td>180.943214</td>\n",
       "      <td>81.146852</td>\n",
       "      <td>59.370289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ahead</th>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>177.796948</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pic</th>\n",
       "      <td>374.744283</td>\n",
       "      <td>100.209638</td>\n",
       "      <td>411.539007</td>\n",
       "      <td>952.493462</td>\n",
       "      <td>177.676277</td>\n",
       "      <td>184.578221</td>\n",
       "      <td>269.945153</td>\n",
       "      <td>161.442305</td>\n",
       "      <td>164.011349</td>\n",
       "      <td>250.060091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>160.137978</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate</th>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>159.628480</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               topic_0     topic_1     topic_2     topic_3     topic_4  \\\n",
       "west          0.100016    0.100044    0.100040  213.052667    0.100018   \n",
       "man           0.100037    0.100008    0.100004    0.100006    0.100010   \n",
       "would         0.100019    0.100009    0.100004    0.100009    0.100015   \n",
       "everything    0.100008    0.100005    0.100003    0.100005    0.100007   \n",
       "someone       0.100010    0.100010    0.100005    0.100007    0.100011   \n",
       "us           80.650146    0.100023   13.109711   52.283461   40.281836   \n",
       "ahead         0.100003    0.100003    0.100009    0.100037    0.100002   \n",
       "pic         374.744283  100.209638  411.539007  952.493462  177.676277   \n",
       "looking       0.100007    0.100005    0.100004    0.100004    0.100012   \n",
       "hate          0.100009    0.100007    0.100004    0.100004    0.100008   \n",
       "\n",
       "               topic_5     topic_6     topic_7     topic_8     topic_9  \n",
       "west        152.841279  101.547642  340.233053   37.983505  205.201675  \n",
       "man           0.100010    0.100011  278.123742    0.100010    0.100007  \n",
       "would         0.100012    0.100014  240.635450    0.100016    0.100007  \n",
       "everything    0.100005    0.100005  232.059128    0.100010    0.100006  \n",
       "someone       0.100012    0.100012  229.937012    0.100018    0.100010  \n",
       "us           30.984174    0.100026  180.943214   81.146852   59.370289  \n",
       "ahead         0.100007    0.100006  177.796948    0.100005    0.100012  \n",
       "pic         184.578221  269.945153  161.442305  164.011349  250.060091  \n",
       "looking       0.100008    0.100008  160.137978    0.100010    0.100006  \n",
       "hate          0.100008    0.100011  159.628480    0.100009    0.100006  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_loadings.sort_values('topic_7', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7776.\n",
      "Testing Score: 0.7815.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslee/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate linear regression model.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "# Fit on Z_train.\n",
    "logreg.fit(lda_train, y_train)\n",
    "\n",
    "# Score on training and testing sets.\n",
    "print(f'Training Score: {round(logreg.score(lda_train, y_train),4)}.')\n",
    "print(f'Testing Score: {round(logreg.score(lda_test, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
